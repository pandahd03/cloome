{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f057fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "src_path = os.path.split(os.getcwd())[0]\n",
    "sys.path.insert(0, src_path)\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import clip.clip as clip\n",
    "from training.datasets import CellPainting\n",
    "from clip.clip import _transform\n",
    "from clip.model import convert_weights, CLIPGeneral\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import DataStructs\n",
    "\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score\n",
    "\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e8a4d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"cloome-retrieval-zero-shot.pt\"\n",
    "REPO_ID = \"anasanchezf/cloome\"\n",
    "checkpoint_path = hf_hub_download(REPO_ID, FILENAME)\n",
    "checkpoint_path = \"/system/user/studentwork/seibezed/cloome/src/logs/retrieval_imgres=[520]_hidden_dim=1024_molecule_layers=4_normalize=dataset_init_inv_tau=14.3_learn_inv_tau=True_lr=0.001_wd=0.1_agg=True_model=RN50_world_size=4batchsize=32_workers=8_date=2025-08-29-18-03-47_/checkpoints/epoch_69.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcb1ca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLOOB\n",
    "model = \"RN50-8192\"\n",
    "image_resolution = 520\n",
    "img_path = \"/system/user/publicdata/cellpainting/npzs/chembl24\"\n",
    "mol_path = \"/system/user/studentwork/seibezed/cloome/src/data/morgan_chiral_fps_8192_test_scaffold.hdf5\"\n",
    "val = \"/publicwork/sanchez_copied/data/murcko_test_imgpermol_final_1398mols.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b23506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_models_to_fp32(model):\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data.float()\n",
    "        if p.grad:\n",
    "            p.grad.data = p.grad.data.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ff3ae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(model_path, device, model, image_resolution):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    state_dict = checkpoint[\"state_dict\"]\n",
    "\n",
    "    model_config_file = os.path.join(src_path, f\"training/model_configs/{model.replace('/', '-')}.json\")\n",
    "    print('Loading model from', model_config_file)\n",
    "    assert os.path.exists(model_config_file)\n",
    "    with open(model_config_file, 'r') as f:\n",
    "        model_info = json.load(f)\n",
    "    model = CLIPGeneral(**model_info)\n",
    "\n",
    "    if str(device) == \"cpu\":\n",
    "        model.float()\n",
    "    print(device)\n",
    "    #print({k for k,v in state_dict.items()})\n",
    "    new_state_dict = {k[len('module.'):]: v for k,v in state_dict.items()}\n",
    "\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model, _transform(image_resolution, image_resolution,  is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0bdefa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(dataset, model, device):\n",
    "    all_image_features = []\n",
    "    all_text_features = []\n",
    "    all_ids = []\n",
    "\n",
    "    print(f\"get_features {device}\")\n",
    "    print(len(dataset))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(DataLoader(dataset, num_workers=20, batch_size=64)):\n",
    "            #print(mols)\n",
    "            imgs, mols = batch\n",
    "\n",
    "            images, mols = imgs[\"input\"], mols[\"input\"]\n",
    "            ids = imgs[\"ID\"]\n",
    "            \n",
    "            img_features = model.encode_image(images.to(device))\n",
    "            text_features = model.encode_text(mols.to(device))\n",
    "\n",
    "            img_features = img_features / img_features.norm(dim=-1, keepdim=True)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            all_image_features.append(img_features)\n",
    "            all_text_features.append(text_features)\n",
    "            all_ids.append(ids)\n",
    "\n",
    "        all_ids = list(chain.from_iterable(all_ids))\n",
    "    return torch.cat(all_image_features), torch.cat(all_text_features), all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae2e1048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df, model_path, model, img_path, mol_path, image_resolution):\n",
    "    # Load the model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(torch.cuda.device_count())\n",
    "\n",
    "    model, preprocess = load(model_path, device, model, image_resolution)\n",
    "\n",
    "    preprocess_train = _transform(image_resolution, image_resolution, is_train=True)\n",
    "    preprocess_val = _transform(image_resolution, image_resolution, is_train=False, normalize=\"dataset\", preprocess=\"crop\")\n",
    "\n",
    "    # Load the dataset\n",
    "    val = CellPainting(df,\n",
    "                       img_path,\n",
    "                       mol_path,\n",
    "                       transforms = preprocess_val)\n",
    "\n",
    "    # Calculate the image features\n",
    "    print(\"getting_features\")\n",
    "    val_img_features, val_text_features, val_ids = get_features(val, model, device)\n",
    "    \n",
    "    return val_img_features, val_text_features, val_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fb42278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(image_features, text_features):\n",
    "    metrics = {}\n",
    "    logits_per_image = image_features @ text_features.t()\n",
    "    logits_per_text = logits_per_image.t()\n",
    "\n",
    "    logits = {\"image_to_text\": logits_per_image, \"text_to_image\": logits_per_text}\n",
    "    ground_truth = (\n",
    "        torch.arange(len(text_features)).view(-1, 1).to(logits_per_image.device)\n",
    "    )\n",
    "\n",
    "    rankings = {}\n",
    "    all_top_samples = {}\n",
    "    all_preds = {}\n",
    "\n",
    "    for name, logit in logits.items():\n",
    "        ranking = torch.argsort(logit, descending=True)\n",
    "        rankings[name] = ranking\n",
    "        preds = torch.where(ranking == ground_truth)[1]\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        all_preds[name] = preds\n",
    "        top_samples = np.where(preds < 10)[0]\n",
    "        all_top_samples[name] = top_samples\n",
    "        metrics[f\"{name}_mean_rank\"] = preds.mean() + 1\n",
    "        metrics[f\"{name}_median_rank\"] = np.floor(np.median(preds)) + 1\n",
    "        for k in [1, 5, 10]:\n",
    "            metrics[f\"{name}_R@{k}\"] = np.mean(preds < k)\n",
    "\n",
    "    return rankings, all_top_samples, all_preds, metrics, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddf5d9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Loading model from /system/user/studentwork/seibezed/cloome/src/training/model_configs/RN50-8192.json\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/system/apps/studentenv/seibezed/cloome/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1398\n",
      "getting_features\n",
      "get_features cuda\n",
      "1398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:31<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "val_img_features, val_text_features, val_ids = main(val, checkpoint_path, model, img_path, mol_path, image_resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb4d3b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings, all_top_samples, all_preds, metrics, logits = get_metrics(val_img_features, val_text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d30d18f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_to_text_mean_rank': 481.9856938483548,\n",
       " 'image_to_text_median_rank': 408.0,\n",
       " 'image_to_text_R@1': 0.031473533619456366,\n",
       " 'image_to_text_R@5': 0.06151645207439199,\n",
       " 'image_to_text_R@10': 0.07439198855507868,\n",
       " 'text_to_image_mean_rank': 490.28111587982835,\n",
       " 'text_to_image_median_rank': 402.0,\n",
       " 'text_to_image_R@1': 0.02861230329041488,\n",
       " 'text_to_image_R@5': 0.060085836909871244,\n",
       " 'text_to_image_R@10': 0.07081545064377683}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a027357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0],\n",
       "        [   1],\n",
       "        [   2],\n",
       "        ...,\n",
       "        [1395],\n",
       "        [1396],\n",
       "        [1397]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = (\n",
    "    torch.arange(len(val_text_features)).view(-1, 1).to(\"cpu\")\n",
    ")\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14a95697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36]\n",
      " [ 0]\n",
      " [14]\n",
      " ...\n",
      " [56]\n",
      " [33]\n",
      " [60]]\n",
      "8.583690987124463 18.812589413447782 26.46638054363376\n",
      "120 263 370\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "\n",
    "for i, logs in enumerate(logits[\"image_to_text\"]):\n",
    "    choices = np.arange(len(val_text_features))\n",
    "    choices = np.delete(choices, i)\n",
    "        \n",
    "    logs = logs.cpu().numpy()\n",
    "    \n",
    "    positive = logs[i]\n",
    "    negatives_ind = np.random.choice(choices, 99, replace=False)\n",
    "    negatives = logs[negatives_ind]\n",
    "    \n",
    "    sampled_logs = np.hstack([positive, negatives])\n",
    "    \n",
    "    ground_truth = np.zeros(len(sampled_logs))\n",
    "    ground_truth[0] = 1\n",
    "    \n",
    "    ranking = np.argsort(sampled_logs)\n",
    "    ranking = np.flip(ranking)\n",
    "    pred = np.where(ranking == 0)[0]\n",
    "    all_preds.append(pred)\n",
    "\n",
    "\n",
    "all_preds = np.vstack(all_preds)\n",
    "print(all_preds)\n",
    "\n",
    "r1 = np.mean(all_preds < 1) * 100\n",
    "r5 = np.mean(all_preds < 5) * 100\n",
    "r10 = np.mean(all_preds < 10) * 100\n",
    "print(r1, r5, r10)\n",
    "\n",
    "n1 = len(np.where(all_preds < 1)[0])\n",
    "n5 = len(np.where(all_preds < 5)[0])\n",
    "n10 = len(np.where(all_preds < 10)[0])\n",
    "print(n1, n5, n10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3932ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43]\n",
      " [ 1]\n",
      " [16]\n",
      " ...\n",
      " [55]\n",
      " [18]\n",
      " [58]]\n",
      "7.582260371959943 16.595135908440632 24.82117310443491\n",
      "106 232 347\n"
     ]
    }
   ],
   "source": [
    "all_preds_t = []\n",
    "\n",
    "for i, logs in enumerate(logits[\"text_to_image\"]):\n",
    "    choices = np.arange(len(val_text_features))\n",
    "    choices = np.delete(choices, i)\n",
    "    \n",
    "    logs = logs.cpu().numpy()\n",
    "    \n",
    "    positive = logs[i]\n",
    "    negatives_ind = np.random.choice(choices, 99, replace=False)\n",
    "    negatives = logs[negatives_ind]\n",
    "    \n",
    "    sampled_logs = np.hstack([positive, negatives])\n",
    "    \n",
    "    ground_truth = np.zeros(len(sampled_logs))\n",
    "    ground_truth[0] = 1\n",
    "    \n",
    "    ranking = np.argsort(sampled_logs)\n",
    "    ranking = np.flip(ranking)\n",
    "    pred = np.where(ranking == 0)[0]\n",
    "    all_preds_t.append(pred)\n",
    "\n",
    "all_preds_t = np.vstack(all_preds_t)\n",
    "print(all_preds_t)\n",
    "\n",
    "r1_t = np.mean(all_preds_t < 1) * 100\n",
    "r5_t = np.mean(all_preds_t < 5) * 100\n",
    "r10_t = np.mean(all_preds_t < 10) * 100\n",
    "print(r1_t, r5_t, r10_t)\n",
    "\n",
    "n1_t = len(np.where(all_preds_t < 1)[0])\n",
    "n5_t = len(np.where(all_preds_t < 5)[0])\n",
    "n10_t = len(np.where(all_preds_t < 10)[0])\n",
    "print(n1_t, n5_t, n10_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0fc7194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "7.081545064377683\n",
      "ConfidenceInterval(low=0.05792469613034303, high=0.08554312389972292)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import binomtest\n",
    "\n",
    "n_samples = 1398\n",
    "value = 0.0708*100\n",
    "\n",
    "successes = int(round(value * n_samples / 100))\n",
    "print(successes)\n",
    "\n",
    "btest = binomtest(k=successes, n=n_samples)\n",
    "result = btest.proportion_estimate * 100\n",
    "ci = btest.proportion_ci(confidence_level=0.95)\n",
    "    \n",
    "print(result)\n",
    "print(ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f78973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
