{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "86c4fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "src_path = os.path.split(os.getcwd())[0]\n",
    "sys.path.insert(0, src_path)\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import clip.clip as clip\n",
    "from training.datasets import CellPainting\n",
    "from clip.clip import _transform\n",
    "from clip.model import convert_weights, CLIPGeneral\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import DataStructs\n",
    "\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score\n",
    "\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0e6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"cloome-retrieval-zero-shot.pt\"\n",
    "REPO_ID = \"anasanchezf/cloome\"\n",
    "checkpoint_path = hf_hub_download(REPO_ID, FILENAME)\n",
    "\n",
    "checkpoint_path = \"/system/user/studentwork/seibezed/cloome/src/logs/imgres=[520]_hidden_dim=1024_molecule_layers=4_normalize=dataset_init_inv_tau=14.3_learn_inv_tau=False_lr=0.001_wd=0.1_agg=True_model=RN50_world_size=4batchsize=32_workers=8_date=2025-09-02-12-15-07_/checkpoints/epoch_63.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ea8e8b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLOOB\n",
    "model = \"RN50\"\n",
    "image_resolution = 520\n",
    "img_path = \"/system/user/publicdata/cellpainting/npzs/chembl24\"\n",
    "mol_path = \"/system/user/studentwork/seibezed/cloome/src/data/morgan_chiral_fps_1024_test_zs_molecules_scaffold.hdf5\"\n",
    "val = \"/publicwork/sanchez_copied/data/cellpainting-test-phenotype-imgpermol-scaffold.csv\"\n",
    "classes = \"/publicwork/sanchez_copied/data/cellpainting-split-test-imgpermol-scaffold.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b32ccf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_models_to_fp32(model):\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data.float()\n",
    "        if p.grad:\n",
    "            p.grad.data = p.grad.data.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8dcceb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(model_path, device, model, image_resolution):\n",
    "\n",
    "    checkpoint = torch.load(model_path)\n",
    "    state_dict = checkpoint[\"state_dict\"]\n",
    "\n",
    "    model_config_file = os.path.join(src_path, f\"training/model_configs/{model.replace('/', '-')}.json\")\n",
    "\n",
    "    print('Loading model from', model_config_file)\n",
    "    assert os.path.exists(model_config_file)\n",
    "    with open(model_config_file, 'r') as f:\n",
    "        model_info = json.load(f)\n",
    "    model = CLIPGeneral(**model_info)\n",
    "\n",
    "    if str(device) == \"cpu\":\n",
    "        model.float()\n",
    "    print(device)\n",
    "\n",
    "    new_state_dict = {k[len('module.'):]: v for k,v in state_dict.items()}\n",
    "\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model, _transform(image_resolution, image_resolution,  is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fd0578e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(dataset, model, device):\n",
    "    all_image_features = []\n",
    "    all_text_features = []\n",
    "    all_ids = []\n",
    "\n",
    "    print(f\"get_features {device}\")\n",
    "    print(len(dataset))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(DataLoader(dataset, num_workers=20, batch_size=64)):\n",
    "            #print(mols)\n",
    "            imgs, mols = batch\n",
    "\n",
    "            images, mols = imgs[\"input\"], mols[\"input\"]\n",
    "            ids = imgs[\"ID\"]\n",
    "            \n",
    "            img_features = model.encode_image(images.to(device))\n",
    "            text_features = model.encode_text(mols.to(device))\n",
    "\n",
    "            img_features = img_features / img_features.norm(dim=-1, keepdim=True)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            all_image_features.append(img_features)\n",
    "            all_text_features.append(text_features)\n",
    "            all_ids.append(ids)\n",
    "\n",
    "        all_ids = list(chain.from_iterable(all_ids))\n",
    "    return torch.cat(all_image_features), torch.cat(all_text_features), all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9c848ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df, model_path, model, img_path, mol_path, image_resolution):\n",
    "    # Load the model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(torch.cuda.device_count())\n",
    "\n",
    "    model, preprocess = load(model_path, device, model, image_resolution)\n",
    "\n",
    "    preprocess_train = _transform(image_resolution, image_resolution, is_train=True)\n",
    "    preprocess_val = _transform(image_resolution, image_resolution, is_train=False, normalize=\"dataset\", preprocess=\"downsize\")\n",
    "\n",
    "    # Load the dataset\n",
    "    val = CellPainting(df,\n",
    "                       img_path,\n",
    "                       mol_path,\n",
    "                       transforms = preprocess_val)\n",
    "\n",
    "    # Calculate the image features\n",
    "    print(\"getting_features\")\n",
    "    val_img_features, val_text_features, val_ids = get_features(val, model, device)\n",
    "    \n",
    "    return val_img_features, val_text_features, val_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "97faec4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Loading model from /system/user/studentwork/seibezed/cloome/src/training/model_configs/RN50.json\n",
      "cuda\n",
      "28473\n",
      "getting_features\n",
      "get_features cuda\n",
      "28473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [03:42<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Loading model from /system/user/studentwork/seibezed/cloome/src/training/model_configs/RN50.json\n",
      "cuda\n",
      "1398\n",
      "getting_features\n",
      "get_features cuda\n",
      "1398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "val_img_features, val_text_features, val_ids = main(val, checkpoint_path, model, img_path, mol_path, image_resolution)\n",
    "class_img_features, class_text_features, class_ids = main(classes, checkpoint_path, model, img_path, mol_path, image_resolution)\n",
    "val_img_features = val_img_features.cpu()\n",
    "class_img_features = class_img_features.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f0db7220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28473, 512])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_img_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "349fe24c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes_df = pd.read_csv(classes)\n",
    "classes_df.set_index(\"SAMPLE_KEY\", inplace=True)\n",
    "class_inchis = classes_df.loc[class_ids][\"INCHIKEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0afde09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv(val)\n",
    "val_df.set_index(\"SAMPLE_KEY\", inplace=True)\n",
    "val_inchis = val_df.loc[val_ids][\"INCHIKEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "711d67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {}\n",
    "\n",
    "for i, inchi in enumerate(class_inchis): \n",
    "    class_dict[inchi] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b908522f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ground_truth = np.zeros(len(val_inchis), dtype=int)\n",
    "\n",
    "for i, inchi in enumerate(val_inchis): \n",
    "    label = class_dict[inchi]\n",
    "    ground_truth[i] = int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ddfa0f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.81716011660169\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracies in several ways\n",
    "\n",
    "# WAY 1\n",
    "logits = val_img_features @ class_img_features.T\n",
    "acc = accuracy_score(ground_truth, logits.argmax(axis=1)) * 100.0\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e6557bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.81716011660169"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WAY 2\n",
    "N = ground_truth.shape[0]\n",
    "(np.array(ground_truth) == logits.argmax(axis=1).numpy()).sum() / N * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "77d9fc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R@1': 21.81716011660169, 'R@5': 49.15534014680575, 'R@10': 65.65518210234256}\n"
     ]
    }
   ],
   "source": [
    "# WAY 3\n",
    "ranking = torch.argsort(logits, descending=True)\n",
    "t = torch.tensor(ground_truth, dtype=torch.int16).view(-1,1)\n",
    "\n",
    "preds = torch.where(ranking == t)[1]\n",
    "preds = preds.detach().cpu().numpy()\n",
    "\n",
    "metrics = {}\n",
    "for k in [1, 5, 10]:\n",
    "    metrics[f\"R@{k}\"] = np.mean(preds < k) * 100\n",
    "    \n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "94297e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R@1': 21.81716011660169, 'R@5': 49.15534014680575, 'R@10': 65.65518210234256}\n"
     ]
    }
   ],
   "source": [
    "# WAY 4\n",
    "probs = (val_img_features @ class_img_features.T).softmax(dim=-1)\n",
    "\n",
    "metrics_skl = {}\n",
    "for k in [1, 5, 10]:\n",
    "    metrics_skl[f\"R@{k}\"] = top_k_accuracy_score(ground_truth, probs, k=k) * 100\n",
    "    \n",
    "print(metrics_skl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4e405617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R@1': 21.81716011660169, 'R@5': 49.15534014680575, 'R@10': 65.65167000316089}\n",
      "{'R@1': ConfidenceInterval(low=0.21338579779160258, high=0.22301497567509646), 'R@5': ConfidenceInterval(low=0.48573011632899354, high=0.4973784106100583), 'R@10': ConfidenceInterval(low=0.6509676026653524, high=0.6620338446570153)}\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import binomtest\n",
    "\n",
    "n_samples = val_img_features.shape[0]\n",
    "\n",
    "mdict, cis = {}, {}\n",
    "\n",
    "for metric, value in metrics.items():\n",
    "    successes = int(value * n_samples / 100)\n",
    "    btest = binomtest(k=successes, n=n_samples)\n",
    "    mdict[metric] = btest.proportion_estimate * 100\n",
    "    cis[metric] = btest.proportion_ci(confidence_level=0.95)\n",
    "    \n",
    "print(mdict)\n",
    "print(cis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
